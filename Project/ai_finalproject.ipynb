{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ai_finalproject.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "taukzMT-H6MP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9ef9262-724c-4094-d3eb-8c22a63dbacc"
      },
      "source": [
        "!pip install hazm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hazm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/13/5a7074bc11d20dbbb46239349ac3f85f7edc148b4cf68e9b8c2f8263830c/hazm-0.7.0-py3-none-any.whl (316kB)\n",
            "\r\u001b[K     |█                               | 10kB 17.0MB/s eta 0:00:01\r\u001b[K     |██                              | 20kB 21.8MB/s eta 0:00:01\r\u001b[K     |███                             | 30kB 26.9MB/s eta 0:00:01\r\u001b[K     |████▏                           | 40kB 24.4MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 51kB 17.3MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 61kB 19.5MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 71kB 12.8MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 81kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 92kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 102kB 15.3MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 112kB 15.3MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 122kB 15.3MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 133kB 15.3MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 143kB 15.3MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 153kB 15.3MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 163kB 15.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 174kB 15.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 184kB 15.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 194kB 15.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 204kB 15.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 215kB 15.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 225kB 15.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 235kB 15.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 245kB 15.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 256kB 15.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 266kB 15.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 276kB 15.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 286kB 15.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 296kB 15.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 307kB 15.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 317kB 15.3MB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/09/3b1755d528ad9156ee7243d52aa5cd2b809ef053a0f31b53d92853dd653a/nltk-3.3.0.zip (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 30.4MB/s \n",
            "\u001b[?25hCollecting libwapiti>=0.2.1; platform_system != \"Windows\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/0f/1c9b49bb49821b5856a64ea6fac8d96a619b9f291d1f06999ea98a32c89c/libwapiti-0.2.1.tar.gz (233kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 43.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n",
            "Building wheels for collected packages: nltk, libwapiti\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-cp37-none-any.whl size=1394490 sha256=161c1cad8d24477d46957b7a692780ff2c1cddae1ca28543821d7a2af6f69015\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/ab/40/3bceea46922767e42986aef7606a600538ca80de6062dc266c\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=154253 sha256=1f19693084b8cbf978fbab85a616a50fda8be1714bfff5af17b811d7a60198f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/15/54/4510dce8bb958b1cdd2c47425cbd1e1eecc0480ac9bb1fb9ab\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, hazm\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDVc5Fg2-g1K"
      },
      "source": [
        "<p align='center'><strong>Imports</strong></p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOHvu7QkFXgT"
      },
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from hazm import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7ZLezhJ-s8y"
      },
      "source": [
        "<p align='center'><strong>Mounting Google Drive</strong></p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iW9oNmLCWli9",
        "outputId": "f1cd0b9e-8522-4062-ae65-67d7ce90ff90"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3uORUQg_O1I"
      },
      "source": [
        "<p align='center'><strong>Reading English, French, Turkish and German, Persian and Arabic Dataset</strong></p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrBnSAQFlLCy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "65a7a287-6322-4937-acec-e940eab95528"
      },
      "source": [
        "all_dataset = pd.read_csv('drive/MyDrive/Artificial Intelligence-Spring 2021/project/all.csv')\n",
        "english_dataset = all_dataset.loc[all_dataset['Language'] == 'English'] \n",
        "german_dataset = all_dataset.loc[all_dataset['Language'] == 'German']\n",
        "french_dataset = all_dataset.loc[all_dataset['Language'] == 'French']\n",
        "turkish_dataset = all_dataset.loc[all_dataset['Language'] == 'Turkish']\n",
        "\n",
        "language_dataset = [english_dataset, german_dataset, french_dataset, turkish_dataset]\n",
        "result = pd.concat(language_dataset)\n",
        "\n",
        "\n",
        "dic = {\n",
        "    'Text': 'Id',\n",
        "    'Language':'Category',\n",
        "}\n",
        "result = result.rename(columns=dic)\n",
        "\n",
        "\n",
        "arabic_dataset = pd.read_csv('drive/MyDrive/Artificial Intelligence-Spring 2021/project/arabic.csv')\n",
        "arabic_dataset = arabic_dataset.head(1800)\n",
        "arabic_dataset.pop('targe')\n",
        "arabic_dataset['Category'] = 'Arabic'\n",
        "dic = {\n",
        "    'text': 'Id',\n",
        "    'Category': 'Category',\n",
        "}\n",
        "arabic_dataset = arabic_dataset.rename(columns=dic)\n",
        "\n",
        "dataset_path = 'drive/MyDrive/Artificial Intelligence-Spring 2021/project/all_language_dataset.csv'\n",
        "\n",
        "result.to_csv(dataset_path, mode='w', index=False)\n",
        "arabic_dataset.to_csv(dataset_path, mode='a', header=False, index=False)\n",
        "\n",
        "persian_dataset = pd.read_csv('drive/MyDrive/Artificial Intelligence-Spring 2021/project/persian.csv')\n",
        "persian_dataset['Category'] = 'Persian'\n",
        "persian_dataset.to_csv(dataset_path, mode='a', index=False, header='False')\n",
        "\n",
        "all_language_dataset = pd.read_csv('drive/MyDrive/Artificial Intelligence-Spring 2021/project/all_language_dataset.csv')\n",
        "\n",
        "# shuffling dataset\n",
        "all_language_dataset = shuffle(all_language_dataset)\n",
        "\n",
        "all_language_dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2408</th>\n",
              "      <td>Le prix a été décerné à Jimmy Wales par David ...</td>\n",
              "      <td>French</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4843</th>\n",
              "      <td>البرايم يستضيف لمنور وحماقي والداودية وباقبو و...</td>\n",
              "      <td>Arabic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>271</th>\n",
              "      <td>[14] It was founded on March 9, 2000, under th...</td>\n",
              "      <td>English</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5517</th>\n",
              "      <td>پليس هند چهار تن از رهبران استقلال طلبان مسلم...</td>\n",
              "      <td>Persian</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4619</th>\n",
              "      <td>البرنامج يبث مباشرة وموجه إلى كل أفراد الأسرة ...</td>\n",
              "      <td>Arabic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1646</th>\n",
              "      <td>nicht viel.</td>\n",
              "      <td>German</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4592</th>\n",
              "      <td>المغني اللبناني قال إنه بصدد إعداد أغنية مغربي...</td>\n",
              "      <td>Arabic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1607</th>\n",
              "      <td>Du kannst sagen, es ist mir egal, das ist so l...</td>\n",
              "      <td>German</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>The field of geology encompasses the study of ...</td>\n",
              "      <td>English</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>362</th>\n",
              "      <td>[89] Editors can enforce these rules by deleti...</td>\n",
              "      <td>English</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5742 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     Id Category\n",
              "2408  Le prix a été décerné à Jimmy Wales par David ...   French\n",
              "4843  البرايم يستضيف لمنور وحماقي والداودية وباقبو و...   Arabic\n",
              "271   [14] It was founded on March 9, 2000, under th...  English\n",
              "5517   پليس هند چهار تن از رهبران استقلال طلبان مسلم...  Persian\n",
              "4619  البرنامج يبث مباشرة وموجه إلى كل أفراد الأسرة ...   Arabic\n",
              "...                                                 ...      ...\n",
              "1646                                        nicht viel.   German\n",
              "4592  المغني اللبناني قال إنه بصدد إعداد أغنية مغربي...   Arabic\n",
              "1607  Du kannst sagen, es ist mir egal, das ist so l...   German\n",
              "30    The field of geology encompasses the study of ...  English\n",
              "362   [89] Editors can enforce these rules by deleti...  English\n",
              "\n",
              "[5742 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6z0FOV_ILmE"
      },
      "source": [
        "<p align='center'><strong>Deleting stopwords</strong></p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9PDf9gwITDf"
      },
      "source": [
        "normalizer = Normalizer()\n",
        "\n",
        "stopwords_list = []\n",
        "stopwords_url = 'drive/MyDrive/Artificial Intelligence-Spring 2021/project/stopwords.txt'\n",
        "f = open(stopwords_url, mode='r')\n",
        "stopwords_list.append(f.read())\n",
        "persian_stopwords = []\n",
        "for i in stopwords_list:\n",
        "  persian_stopwords.append(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8XgpU8rmzzi"
      },
      "source": [
        "# with open('dataset.csv', 'w', newline='') as file:\n",
        "#   writer = csv.writer(file)\n",
        "#   writer.writerow(['Id', 'Category'])\n",
        "\n",
        "# dataset = pd.read_csv('dataset.csv')\n",
        "\n",
        "# creating a csv file for final dataset\n",
        "# with open(dataset_path, mode='w', encoding='utf-8') as dataset_file:\n",
        "#   wr = csv.writer(dataset_file, delimiter=',')\n",
        "#   wr.writerow(['Id', 'Category'])\n",
        "#   for (Id, Category) in enumerate(result):\n",
        "#     wr.writerow((Id, Category))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kReYfKPS7n0-"
      },
      "source": [
        "with open('drive/MyDrive/Artificial Intelligence-Spring 2021/project/persian.txt', 'r', encoding=\"utf-8\") as f:\n",
        "  lines = f.readlines()\n",
        "\n",
        "with open('persiantext.csv', 'w', encoding=\"utf-8\") as f:\n",
        "  for line in lines:\n",
        "      if not line.strip('\\n').startswith(\"# File: www\") \\\n",
        "          and not line.strip('\\n').startswith(\"# Date\") \\\n",
        "          and not line.strip('\\n').startswith(\"# Headline\"):\n",
        "        f.write(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzL2cUsyLYh8",
        "outputId": "abdeaea9-c217-41a3-f473-60597be7d664"
      },
      "source": [
        "print(len(all_language_dataset.loc[all_language_dataset['Category'] == 'French']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1014\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DUKOLX6tl64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b2d772d-fcc2-4545-e2af-8ae44201a0a2"
      },
      "source": [
        "categories = all_language_dataset[1].unique()\n",
        "categories"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Turkish', 'English', 'Arabic', 'French', 'Persian', 'German',\n",
              "       'Category'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeEYEPaottvi"
      },
      "source": [
        "for i in categories:\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_JVCymbGWOa"
      },
      "source": [
        "<p align='center'><strong>Vectorizing the Words.</strong></p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap9KEn85t_Ca"
      },
      "source": [
        "kaggle_task = pd.read_csv('drive/MyDrive/Artificial Intelligence-Spring 2021/project/task1.csv')\n",
        "help_me = kaggle_task.Id\n",
        "\n",
        "vectorizer = TfidfVectorizer(min_df=11, max_df=0.07, decode_error='strict', ngram_range=(1, 2), binary=0.5, sublinear_tf=True)\n",
        "train_x = vectorizer.fit_transform(all_language_dataset.Id)\n",
        "labelencoder = LabelEncoder()   \n",
        "train_y = labelencoder.fit_transform(all_language_dataset.Category)\n",
        "\n",
        "kaggle_task_x = vectorizer.transform(kaggle_task.Id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rm-ZhsZ-Ghfg"
      },
      "source": [
        "<p align='center'><strong>Train-Test split test.</strong></p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh2acAvnu-Gi"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(train_x, train_y, test_size=0.25, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nGwZB6cGnNR"
      },
      "source": [
        "<p align='center'><strong>Building Model</strong></p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OL-73GPYvaQl",
        "outputId": "e600e1ca-65f6-4269-9aa0-7194214e0f19"
      },
      "source": [
        "def benchmark(clf):\n",
        "  print(\"**\", clf, \"***\")\n",
        "  clf.fit(x_train, y_train)\n",
        "  prediction = clf.predict(x_test)\n",
        "  score = metrics.accuracy_score(y_test, prediction)\n",
        "  print(\"accuracy:  %0.3f\",  score)\n",
        "\n",
        "results = []\n",
        "clf = SGDClassifier(loss=\"modified_huber\", max_iter=1500, shuffle=False)\n",
        "name = \"SGD Classifier\"\n",
        "print(name)\n",
        "results.append(benchmark(clf))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SGD Classifier\n",
            "** SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
            "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
            "              l1_ratio=0.15, learning_rate='optimal', loss='modified_huber',\n",
            "              max_iter=1500, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
            "              power_t=0.5, random_state=None, shuffle=False, tol=0.001,\n",
            "              validation_fraction=0.1, verbose=0, warm_start=False) ***\n",
            "accuracy:  %0.3f 0.9811977715877437\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaOlPPaRGqWq"
      },
      "source": [
        "<p align='center'><strong>Prediction on test data.</strong></p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_jK3QN3vI83",
        "outputId": "33d472d4-5c4d-475f-87d6-cdc2861dfd79"
      },
      "source": [
        "kaggle_results = []\n",
        "print(\"***  KAGGLE TEST  ***\")\n",
        "classifier = SGDClassifier(loss=\"modified_huber\", max_iter=1500, shuffle=False)\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        "kaggle_prediction = classifier.predict(kaggle_task_x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***  KAGGLE TEST  ***\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNsDvpmEGuTl"
      },
      "source": [
        "<p align='center'><strong>Writing results on the file</strong></p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmY4OlhmvMsl"
      },
      "source": [
        "preLabels = ['Id', 'Category']\n",
        "labels_y = labelencoder.inverse_transform(kaggle_prediction)\n",
        "labels_x = vectorizer.inverse_transform(kaggle_task_x)\n",
        "\n",
        "\n",
        "result_file = 'drive/MyDrive/Artificial Intelligence-Spring 2021/project/result22.csv'\n",
        "with open(result_file, mode='w') as result_file:\n",
        "  wr = csv.writer(result_file, delimiter=',')\n",
        "  wr.writerow(preLabels)\n",
        "  for (id, item) in enumerate(labels_y, start=0):\n",
        "    wr.writerow((help_me[id] , item))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKrZYAYniYcP"
      },
      "source": [
        "print(train_x[0])\n",
        "y1 = np.zeros([train_x.shape[0], 6])\n",
        "y1 = pd.DataFrame(y1)\n",
        "y1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiNa8XCog1Di"
      },
      "source": [
        "\n",
        "def hypothesis(theta, X):\n",
        "    return 1 / (1 + np.exp(-(np.dot(theta, X.T)))) - 0.0000001\n",
        "\n",
        "def cost(X, y, theta):\n",
        "    y1 = hypothesis(X, theta)\n",
        "    return -(1/len(X)) * np.sum(y*np.log(y1) + (1-y)*np.log(1-y1))\n",
        "\n",
        "def gradient_descent(X, y, theta, alpha, epochs):\n",
        "    m = len(all_language_dataset)\n",
        "    for i in range(0, epochs):\n",
        "        for j in range(0, 10):\n",
        "            theta = pd.DataFrame(theta)\n",
        "            h = hypothesis(theta.iloc[:,j], X)\n",
        "            for k in range(0, theta.shape[0]):\n",
        "                theta.iloc[k, j] -= (alpha/m) * np.sum((h-y.iloc[:, j])*X.iloc[:, k])\n",
        "            theta = pd.DataFrame(theta)\n",
        "    return theta, cost\n",
        "\n",
        "\n",
        "theta = np.zeros([train_x.shape[1]+1, y1.shape[1]])\n",
        "theta = gradient_descent(train_x, y1, theta, 0.02, 1500)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}